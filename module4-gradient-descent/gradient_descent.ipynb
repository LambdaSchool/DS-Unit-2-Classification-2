{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VDv2BqsPTpn",
        "colab_type": "text"
      },
      "source": [
        "_Lambda School Data Science_\n",
        "\n",
        "# Gradient Descent\n",
        "\n",
        "#### Objectives\n",
        "- Understand the concept of how gradient descent works and why it is useful\n",
        "- Implement and visualize gradient descent to solve linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k89MMrvUPTpo",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://twitter.com/dorrismccomics/status/1022803306173923328\"><img src=\"https://pbs.twimg.com/media/DjG6LapXcAAQ46q?format=jpg\" alt=\"oh no\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk1SPda1PTpo",
        "colab_type": "text"
      },
      "source": [
        "## 3Blue1Brown, [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfLu73d8PTpp",
        "colab_type": "text"
      },
      "source": [
        "It's provocative to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise and a lot more like, well, a calculus exercise. Basically it comes down to finding the minimum of a certain function. \n",
        "\n",
        "You define a cost function: a way of telling the computer: \"No, bad computer!\" But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to _change_ those weights and biases, so that it gets better.\n",
        "\n",
        "Just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqlvtPw5PTpq",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.imgur.com/sEWIS5I.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcfstLVMPTpq",
        "colab_type": "text"
      },
      "source": [
        "Calculus students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really complicated functions.\n",
        "\n",
        "A more flexible tactic is to start at any old input and figure out which direction you should step to make that output lower.\n",
        "\n",
        "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.\n",
        "\n",
        "If you do this repeatedly at each point, checking the new slope and taking the appropriate step, you're gonna approach some local minimum of the function.\n",
        "\n",
        "---\n",
        "\n",
        "Bumping up the complexity a bit, imagine instead a function with _two_ inputs and one output.\n",
        "\n",
        "Now instead of asking about the slope of the function, you have to ask which direction should you step, to decrease the output of the function most quickly? In other words, what's the downhill direction? It's helpful to think of a ball rolling down that hill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHXCjmFaPTpr",
        "colab_type": "text"
      },
      "source": [
        "![](https://media.giphy.com/media/cLGkK33brqidKXS8dS/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2sidJjlPTpr",
        "colab_type": "text"
      },
      "source": [
        "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent. Taking the negative of that gradient gives you the direction to step that decreases the function most quickly\n",
        "\n",
        "If you're unfamiliar with multivariable calculus, honestly, all that matters for you and me right now is that there exists a way to compute this downhill direction. You'll be okay if that's all you know and you're not rock solid on the details.\n",
        "\n",
        "Because if you can get that, the algorithm for minimizing the function is to \n",
        "\n",
        "- compute this gradient direction, \n",
        "- then take a small step downhill, \n",
        "- and just repeat that over and over. \n",
        "\n",
        "It's the same basic idea for a function that has 13,000 inputs instead of two inputs. This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS9e-skTPTps",
        "colab_type": "text"
      },
      "source": [
        "## Let's code it!\n",
        "\n",
        "> You define a cost function: a way of telling the computer: \"No, bad computer!\" But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to _change_ those weights and biases, so that it gets better.\n",
        ">\n",
        "> Just imagine a simple function that has one number as an input and one number as an output. \n",
        "\n",
        "Let's plot a function like that: $f(x) = x^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3K8usxVPTpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I680E0QoPTpw",
        "colab_type": "text"
      },
      "source": [
        "> How do you find an input that minimizes the value of this function?\n",
        ">\n",
        "> You can sometimes figure out that minimum explicitly. But that's not always feasible for really complicated functions.\n",
        ">\n",
        "> A more flexible tactic is to start at any old input ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVbNvBtnPTpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPwyfpzRPTpz",
        "colab_type": "text"
      },
      "source": [
        "> Start at any old input and figure out which direction you should step to make that output lower.\n",
        ">\n",
        "> Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv8ptOYfPTp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbjqEt36PTp2",
        "colab_type": "text"
      },
      "source": [
        "> If you do this repeatedly at each point, checking the new slope and taking the appropriate step, you're gonna approach some local minimum of the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq7tpk-pPTp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeXNNPOmPTp5",
        "colab_type": "text"
      },
      "source": [
        "> The algorithm for minimizing the function is to \n",
        ">\n",
        "> - compute this gradient direction, \n",
        "> - then take a small step downhill, \n",
        "> - and just repeat that over and over. \n",
        ">\n",
        "> This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfhlaSBYPTp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQPSNHimPTqA",
        "colab_type": "text"
      },
      "source": [
        "## NEXT: Solving Linear Regression with Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtB9tuZPTqA",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the first quadrant of Anscombe's quartet. \n",
        "\n",
        "We can draw a line to fit the data, adjust the line's slope, and visualize the cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "p4bOFs_mPTqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUwuG344PTqD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "***We can use Gradient Descent to solve Linear Regression. Here's how:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U6IEKoDPTqD",
        "colab_type": "text"
      },
      "source": [
        "## Andrew Ng, [Gradient Descent for Linear Regression](https://www.coursera.org/lecture/machine-learning/gradient-descent-for-linear-regression-kCvQc) with One Variable\n",
        "\n",
        "> We're gonna put together gradient descent with our squared error cost function, and that will give us an algorithm for linear regression.\n",
        ">\n",
        "> In order to apply gradient descent, we need the partial derivative terms. Computing those partial derivative terms requires some multivariate calculus. If you know calculus, feel free to work through the derivations yourself and check that if you take the derivatives, you actually get the answers that I got. But if you're less familiar with calculus, don't worry about it and it's fine to just take these equations that were worked out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ke83Z7PTqE",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.imgur.com/g9P2Pod.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09LC9fr6PTqE",
        "colab_type": "text"
      },
      "source": [
        "#### Let's translate this math notation into Python code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcxZ_QEWPTqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK1g_BCuPTqG",
        "colab_type": "text"
      },
      "source": [
        "#### Then use our code to solve linear regression problems with one variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4hJqQoWPTqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trAecamVPTqL",
        "colab_type": "text"
      },
      "source": [
        "# ASSIGNMENT\n",
        "\n",
        "Choose of these options, and ***share your work with your cohort on Slack!***\n",
        "\n",
        "### Visualize\n",
        "- How can you adapt or improve these visualizations?\n",
        "   - Could you draw tangent lines in the cost function visualizations?\n",
        "   - Could you create an interactive visualization of gradient descent for linear regression?\n",
        "   - Could you explain these concepts in different ways, by creating different visualizations?\n",
        "- [Lambda School DS2 student Cole Hudson created an awesome visualization!](https://gist.github.com/colejhudson/5cb08e9c51d1600a799deb25758a1c61) He implemented gradient descent to optimize a complex [\"Rosenbrock function\"](https://en.wikipedia.org/wiki/Rosenbrock_function), and visualized it in 3D. What can you learn from this example? What will you be inspired to create?\n",
        "\n",
        "### Implement\n",
        "- We implemented gradient descent for linear regression with one variable. Can you extend the code to work for multiple regression?\n",
        "- Choose another regression dataset. Fit the data with your gradient descent code, and without. (For example, use scikit-learn's Linear Regression model.) Compare your results.\n",
        "\n",
        "### Understand\n",
        "Explore another explanation or tutorial for Gradient Descent. You can use these links, or find another link of your choice. Share a quote or image from your link to show what you learn.\n",
        "\n",
        "- [Andrew Ng, Gradient Descent](https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM)\n",
        "- [Brandon Rohrer, How Optimization Works](https://brohrer.github.io/how_optimization_works_1.html)\n",
        "- [Joel Grus, Data Science from Scratch, 2nd Edition](https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/), Chapter 8: Gradient Descent\n",
        "- [Kalid Azad, Understanding the Gradient](https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/)\n",
        "- [Siraj Raval, The Evolution of Gradient Descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- [Terence Parr, Jeremy Howard, How to explain gradient boosting](https://explained.ai/gradient-boosting/)\n",
        "\n",
        "### Explore more model interpretation visualizations\n",
        "Recently we looked at just a few options in a few Python libraries for model interpretation visualizations. Explore more options to create new visuals in new ways. This isn't about Gradient Descent, but would be a great use of your time too!\n",
        "\n",
        "- https://github.com/TeamHG-Memex/eli5\n",
        "- https://github.com/slundberg/shap\n",
        "- https://github.com/oracle/Skater"
      ]
    }
  ]
}